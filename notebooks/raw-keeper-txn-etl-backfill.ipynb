{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dba440df-183d-4e06-89fb-e18c38279c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from google.cloud import storage\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791421e5-5ab3-4309-b4cd-2906a09bbac9",
   "metadata": {},
   "source": [
    "# Utility Function to Execute BitQuery Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2d9f559-06ff-4e72-815f-3e6fe948b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, retries=10):\n",
    "        \"\"\"\n",
    "        Query graphQL API.\n",
    "        If timeerror\n",
    "        \"\"\"\n",
    "        headers = {\"X-API-KEY\": \"BQYCaXaMZlqZrPCSQVsiJrKtxKRVcSe4\"}\n",
    "\n",
    "        retries_counter = 0\n",
    "        try:\n",
    "            request = requests.post(\n",
    "                \"https://graphql.bitquery.io/\", json={\"query\": query}, headers=headers\n",
    "            )\n",
    "            result = request.json()\n",
    "            # print(dir(request.content))\n",
    "            # Make sure that there is no error message\n",
    "            # assert not request.content.errors\n",
    "            assert \"errors\" not in result\n",
    "        except:\n",
    "            while (\n",
    "                (request.status_code != 200\n",
    "                or \"errors\" in result)\n",
    "                and retries_counter < 10\n",
    "            ):\n",
    "                print(datetime.now(), f\"Retry number {retries_counter}\")\n",
    "                if \"errors\" in result:\n",
    "                    print(result[\"errors\"])\n",
    "                print(datetime.now(), f\"Query failed for reason: {request.reason}. sleeping for {150*retries_counter} seconds and retrying...\")\n",
    "                time.sleep(150*retries_counter)\n",
    "                request = requests.post(\n",
    "                    \"https://graphql.bitquery.io/\",\n",
    "                    json={\"query\": query},\n",
    "                    headers=headers,\n",
    "                )\n",
    "                retries_counter += 1\n",
    "            if retries_counter >= retries:\n",
    "                raise Exception(\n",
    "                    \"Query failed after {} retries and return code is {}.{}\".format(\n",
    "                        retries_counter, request.status_code, query\n",
    "                    )\n",
    "                )\n",
    "        return request.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb21bcac-ccb9-46ea-b45d-6365a5f81138",
   "metadata": {},
   "source": [
    "# Dictionary Mapping data.base58 to Instruction Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3b2ea93-2972-47bd-9102-7cd8d68c8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_type_dict = {\n",
    "    '5QCjN' : 'CancelAllPerpOrders',\n",
    "    'BNuyR' : 'CachePrices',\n",
    "    'BcYfW' : 'PlacePerpOrder',\n",
    "    'CruFm' : 'CacheRootBanks',\n",
    "    'HRDyP' : 'ConsumeEvents',\n",
    "    'QioWX' : 'CachePerpMarkets',\n",
    "    'SCnns' : 'UpdateFunding',\n",
    "    'Y8jvF' : 'UpdateRootBank',\n",
    "    '' : ''\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4faae8-4dce-4071-bc72-b7c0f58cef2a",
   "metadata": {},
   "source": [
    "# Instantiate Google Cloud Storage Client and Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "133fd904-01db-45af-b2e0-41a2521da339",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket('entropy-keeper-transactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7577d-7f9d-485b-a31b-567b461c977c",
   "metadata": {},
   "source": [
    "# Read in BitQuery Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b1df039a-eef4-4f8f-b51c-d72bdfc6fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../entropy_instructions_bitquery.txt') as query:\n",
    "    query_string = query.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae5984-d82b-47b7-aa21-629345c633bc",
   "metadata": {},
   "source": [
    "# Initialize Starting Time Interval to Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d98e4c20-ec91-42bb-9171-887aa379e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "after = datetime(2022, 5, 4, 0, 0, 0)\n",
    "till = after + timedelta(minutes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcad7f-9a8f-47d6-8279-596afd807a17",
   "metadata": {},
   "source": [
    "# Loop Through Query Results and Write to Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780d2c4-b9f6-4fbf-98ac-686393217929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 23:00:00.146917 Running from 2022-04-22T12:30:00Z to 2022-04-22T13:00:00Z\n",
      "2022-05-01 23:00:00.795833 Query completed!\n",
      "2022-05-01 23:00:00.796984 Creating empty parquet file...\n",
      "2022-05-01 23:00:00.799323 Uploading file to GCS...\n",
      "2022-05-01 23:00:01.058215 Deleting file from local memory\n",
      "2022-05-01 23:00:01.058845 Running from 2022-04-22T13:00:00Z to 2022-04-22T13:30:00Z\n",
      "2022-05-01 23:00:07.367894 Query completed!\n",
      "2022-05-01 23:00:07.424377 Creating parquet file...\n",
      "2022-05-01 23:00:07.430805 Writing data to GBQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10672.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 23:00:15.116095 Uploading file to GCS...\n",
      "2022-05-01 23:00:15.399688 Deleting file from local memory\n",
      "2022-05-01 23:00:15.399993 Running from 2022-04-22T13:30:00Z to 2022-04-22T14:00:00Z\n",
      "2022-05-01 23:00:21.832574 Query completed!\n",
      "2022-05-01 23:00:21.890430 Creating parquet file...\n",
      "2022-05-01 23:00:21.896770 Writing data to GBQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11491.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 23:00:26.938681 Uploading file to GCS...\n",
      "2022-05-01 23:00:27.222765 Deleting file from local memory\n",
      "2022-05-01 23:00:27.223439 Running from 2022-04-22T14:00:00Z to 2022-04-22T14:30:00Z\n",
      "2022-05-01 23:00:28.848985 Query completed!\n",
      "2022-05-01 23:00:28.855458 Creating parquet file...\n",
      "2022-05-01 23:00:28.857864 Writing data to GBQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7913.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-01 23:00:33.042948 Uploading file to GCS...\n",
      "2022-05-01 23:00:33.319362 Deleting file from local memory\n",
      "2022-05-01 23:00:33.319981 Running from 2022-04-22T14:30:00Z to 2022-04-22T15:00:00Z\n",
      "2022-05-01 23:00:34.930985 Query completed!\n",
      "2022-05-01 23:00:34.931542 Creating empty parquet file...\n",
      "2022-05-01 23:00:34.933413 Uploading file to GCS...\n",
      "2022-05-01 23:00:35.095430 Deleting file from local memory\n",
      "2022-05-01 23:00:35.095996 Running from 2022-04-22T15:00:00Z to 2022-04-22T15:30:00Z\n",
      "2022-05-01 23:00:36.761233 Query completed!\n",
      "2022-05-01 23:00:36.768221 Creating parquet file...\n",
      "2022-05-01 23:00:36.770959 Writing data to GBQ...\n"
     ]
    }
   ],
   "source": [
    "while after < datetime(2022, 5, 5, 3, 30, 0):\n",
    "    \n",
    "    after_param = after.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    till_param = till.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    print(datetime.now(),'Running from {} to {}'.format(after_param, till_param))\n",
    "        \n",
    "    result = run_query(query_string % (after_param, till_param))\n",
    "    print(datetime.now(), 'Query completed!')\n",
    "    df = pd.json_normalize(result['data']['solana']['instructions'])\n",
    "    \n",
    "    if df.empty:\n",
    "        print(datetime.now(), 'Creating empty parquet file...')   \n",
    "        df.to_parquet(after_param+'-'+till_param+'.parquet')\n",
    "\n",
    "        print(datetime.now(), 'Uploading file to GCS...')\n",
    "        blob = bucket.blob('raw/'+datetime.strptime(after_param,'%Y-%m-%dT%H:%M:%SZ').date().strftime('%Y-%m-%d')+'/'+after_param+'-'+till_param+'.parquet')\n",
    "        blob.upload_from_filename(after_param+'-'+till_param+'.parquet')\n",
    "\n",
    "        print(datetime.now(), 'Deleting file from local memory')\n",
    "        os.remove(after_param+'-'+till_param+'.parquet')\n",
    "        \n",
    "        after += timedelta(minutes=10)\n",
    "        till += timedelta(minutes=10)\n",
    "    \n",
    "    else:\n",
    "        df['data.base58_trunc'] = df['data.base58'].apply(lambda x: x[:5])\n",
    "        df['instruction_type'] = df['data.base58_trunc'].apply(lambda x: instruction_type_dict[x] if x in instruction_type_dict.keys() else 'other')\n",
    "\n",
    "        df_reduced = df[df['instruction_type'].isin(['UpdateRootBank','CacheRootBanks','CachePerpMarkets','CachePrices','UpdateFunding','ConsumeEvents'])][['block.height','block.timestamp.iso8601','transaction.feePayer','instruction_type','transaction.signature']]\n",
    "        df_reduced.rename(columns={\n",
    "            'transaction.feePayer' : 'entropy_keeper_address',\n",
    "            'transaction.signature' : 'transaction_id'\n",
    "        }, inplace=True)\n",
    "\n",
    "        print(datetime.now(), 'Creating parquet file...')       \n",
    "        df_reduced.to_parquet(after_param+'-'+till_param+'.parquet')\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(datetime.now(), 'Writing data to GBQ...')\n",
    "        df_reduced.rename(columns={\n",
    "            'block.height' : 'block_height',\n",
    "            'block.timestamp.iso8601' : 'block_timestamp_iso8601'\n",
    "        }).to_gbq('solana.entropy_keeper_transactions',if_exists='append')\n",
    "\n",
    "        print(datetime.now(), 'Uploading file to GCS...')\n",
    "        blob = bucket.blob('raw/'+after.date().strftime('%Y-%m-%d')+'/'+after_param+'-'+till_param+'.parquet')\n",
    "        blob.upload_from_filename(after_param+'-'+till_param+'.parquet')\n",
    "\n",
    "        print(datetime.now(), 'Deleting file from local memory')\n",
    "        os.remove(after_param+'-'+till_param+'.parquet')\n",
    "\n",
    "        after += timedelta(minutes=10)\n",
    "        till += timedelta(minutes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a64436-add4-48aa-b6f6-f051d321e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
